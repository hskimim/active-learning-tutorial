{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx_dict = load_obj('english_mr_word_to_idx_dict')\n",
    "idx_to_word_dict = {idx:word for (word,idx) in word_to_idx_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([39720, 100]), torch.Size([9986, 100]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = torch.load('english_mv_train_X.pt')\n",
    "train_y = torch.load('english_mv_train_y.pt')\n",
    "test_X = torch.load('english_mv_test_X.pt')\n",
    "test_y = torch.load('english_mv_test_y.pt')\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 100]), torch.Size([48706, 100]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.cat((train_X[1000:],test_X)); test_y = torch.cat((train_y[1000:],test_y))\n",
    "train_X = train_X[:1000,:]; train_y = train_y[:1000]\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 10}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(train_X,train_y)\n",
    "train_iter = data.DataLoader(training_set, **params)\n",
    "\n",
    "testing_set = Dataset(test_X,test_y)\n",
    "test_iter = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE ,KERNEL_SIZE, NUM_FILTER, N_CLASS ) : \n",
    "        super(CNN, self).__init__()\n",
    "        self.vocab_size = VOCAB_SIZE \n",
    "        self.embed_size = EMBED_SIZE \n",
    "        self.hid_size = HID_SIZE \n",
    "        self.dropout = DROPOUT \n",
    "        self.batch_size = BATCH_SIZE\n",
    "        if type(KERNEL_SIZE) !=list :\n",
    "            self.kernel_size = list(KERNEL_SIZE)\n",
    "        else : self.kernel_size = KERNEL_SIZE \n",
    "        self.num_filter = NUM_FILTER \n",
    "        self.num_class = N_CLASS \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = self.vocab_size,\n",
    "            embedding_dim = self.embed_size,\n",
    "            padding_idx = 1) \n",
    "        \n",
    "        self.convs = nn.ModuleList([(nn.Conv2d(in_channels = 1,out_channels = self.num_filter,\\\n",
    "        kernel_size = (kernel,self.embed_size))) for kernel in self.kernel_size])\n",
    "        \n",
    "        self.fully_connect = nn.Sequential(\n",
    "        nn.Linear(self.num_filter * len(self.kernel_size),self.hid_size),nn.ReLU(),\n",
    "        nn.Dropout(self.dropout),nn.Linear(self.hid_size , self.num_class),\n",
    "        )\n",
    "        \n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        for conv in self.convs : \n",
    "            torch.nn.init.xavier_uniform_(conv.weight)\n",
    "        \n",
    "    def forward(self,x) : \n",
    "        embed = self.embedding(x) \n",
    "        embed = embed.unsqueeze(1)\n",
    "                \n",
    "        convolution = [conv(embed).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv,(conv.size(2))).squeeze(2) for conv in convolution]\n",
    "        \n",
    "        dropout = [F.dropout(pool,self.dropout) for pool in pooled]\n",
    "        \n",
    "        concatenate = torch.cat(dropout, dim = 1) \n",
    "        # [batch_size , num_filter * num_kernel]\n",
    "\n",
    "        logit = self.fully_connect(concatenate)\n",
    "        \n",
    "        return torch.log_softmax(logit,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_idx_dict)\n",
    "EMBED_SIZE = 256\n",
    "HID_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 100\n",
    "KERNEL_SIZE = [2,3,4,5]\n",
    "NUM_FILTER = 4\n",
    "N_CLASS = 2\n",
    "\n",
    "model = CNN(VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE, KERNEL_SIZE, NUM_FILTER, N_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_softmax = model(next(iter(train_iter))[0].cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig and Testing with AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryStrategy() : \n",
    "        \n",
    "    def least_confidence(self,x,k=10,score=False) :\n",
    "\n",
    "        max_idx_ls = x.max(1)[0].cpu().data.numpy()\n",
    "        \n",
    "        if score : \n",
    "            return max_idx_ls\n",
    "        return list(np.argsort(1-max_idx_ls)[-k:])\n",
    "\n",
    "    def margin_sampling(self,x,k=10,score=False) : \n",
    "\n",
    "        sorted_x = torch.sort(x,dim=1)[0]\n",
    "\n",
    "        margin = sorted_x[:,-1] - sorted_x[:,-2]\n",
    "\n",
    "        if score : \n",
    "            return margin\n",
    "        \n",
    "        return list(np.argsort(margin.cpu().data.numpy())[-k:])\n",
    "\n",
    "    def entropy(self,x,k=10,score=False) : \n",
    "\n",
    "        entropy_ls = []\n",
    "\n",
    "        def calc_entropy(softmax) : \n",
    "\n",
    "            entropy = 0\n",
    "\n",
    "            for x in softmax : \n",
    "                entropy += -1 * x.cpu().data.numpy() * np.log2(x.cpu().data.numpy())\n",
    "\n",
    "            return entropy\n",
    "\n",
    "        for i in x : \n",
    "            entropy_ls.append(calc_entropy(i))\n",
    "            \n",
    "        if score : \n",
    "            return entropy_ls\n",
    "        \n",
    "        return list(np.argsort(np.array(entropy_ls))[-k:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_loader(train_loader,test_loader,idx_ls) : \n",
    "    \n",
    "    X = torch.cat((train_loader.dataset.X,test_loader.dataset.X[idx_ls]),dim=0)\n",
    "    y = torch.cat((train_loader.dataset.y,test_loader.dataset.y[idx_ls]),dim=0)\n",
    "    \n",
    "    params = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    training_set = Dataset(X,y)\n",
    "    train_iter = data.DataLoader(training_set, **params)\n",
    "    \n",
    "    return train_iter\n",
    "\n",
    "def pop_to_loader(loader,query_ls) : \n",
    "    \n",
    "    orgin_idx_ls = [i for i in range(loader.dataset.X.shape[0])]\n",
    "    \n",
    "    idx_ls = []\n",
    "    for val in orgin_idx_ls : \n",
    "        if val not in query_ls : \n",
    "            idx_ls.append(val)\n",
    "            \n",
    "    X = loader.dataset.X[idx_ls]\n",
    "    y = loader.dataset.y[idx_ls]\n",
    "    \n",
    "    params = {'batch_size': 100,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    # test_iter 는 shuffle 을 해서는 안됩니다. 그 이유는 validation datasets 에 대해서 softmax 값을 순차적으로 나열하고,\n",
    "    # 이에 따라, k-means 를 적용 및 informative instance를 쿼리하기 때문에, sequence 정보를 보존해야 합니다.\n",
    "    # 또한, 해당 데이터셋은 training 에는 적용되는 데이터가 아니기 때문에, shuffle 을 하지 않아도 무관합니다.\n",
    "    \n",
    "    testing_set = Dataset(X,y)\n",
    "    iter_ = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    return iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diverse Mini-Batch Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘 정의\n",
    "- 전체 데이터 셋에 대한 softmax 값을 저장\n",
    "- pre-filter 데이터셋 확보 $\\beta k > k$\n",
    "- weighted-kmeans 를 계산 (sklearn Kmeans 사용)\n",
    "- centriod 와 각 instance 간의 distance 를 Euclidean distance를 통해 계산 및 sorting\n",
    "- 상위 $k$ 개의 인스턴스 index를 반환\n",
    "_________________________\n",
    "- $\\beta = 50$\n",
    "- $k = 100$\n",
    "- 이에 따라 $\\beta k$ 즉, 1000개를 pre-filter 한다.\n",
    "- 논문에서 $\\beta$ 가 10일 경우, robust 하다고 했지만, Experiment results에서 MNIST 의 경우, CNN 아키텍처를 사용하고, 데이터 갯수도 많아졌을 경우, $\\beta = 50$이 10일때보다 성능이 좋다고 하여, 50을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, lr = 0.01, batch_size = 100, epoch = 10) :\n",
    "    \n",
    "    query = QueryStrategy()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    criterion_cpu = nn.NLLLoss()\n",
    "    \n",
    "    early_bird_ls = [0.0]\n",
    "    early_bird_count = 0\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    for _ in range(20) : \n",
    "        iteration += 1\n",
    "        val_batch_count = 0; batch_count = 0\n",
    "        n_correct = 0; val_n_correct = 0\n",
    "        score_ls = []\n",
    "        \n",
    "        train_length = train_loader.dataset.X.shape[0]\n",
    "        test_length = test_loader.dataset.X.shape[0]\n",
    "        with open(\"english_random_sampling.txt\",'a') as f : \n",
    "            f.write(\"training 데이터셋의 크기 : {} , validation 데이터셋의 크기 : {}\\n\".format(train_length,test_length))\n",
    "            f.write(\"###################################################################\\n\")\n",
    "        \n",
    "        if train_length > (train_length + test_length) * 0.6 : \n",
    "            with open(\"english_random_sampling.txt\",'a') as f : \n",
    "                f.write(\"training 데이터셋의 크기가 전체의 60%가 넘었기 때문에, iteration 을 종료합니다.\\n\")\n",
    "                \n",
    "            return\n",
    "        \n",
    "        model.train()        \n",
    "        \n",
    "        for time in range(epoch) : \n",
    "            for train_batch, train_labels in train_loader:  \n",
    "\n",
    "                batch_count += 1 \n",
    "                if batch_count % 100 == 0 and batch_count != 0: \n",
    "                    print(\"{}번째 Training 배치가 돌고 있습니다.\".format(batch_count),end='\\r')\n",
    "\n",
    "                train_softmax = model(train_batch.to(device))\n",
    "                train_predict = train_softmax.argmax(dim=1)     \n",
    "\n",
    "                loss = criterion(train_softmax,train_labels.to(device))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if time == epoch - 1 : \n",
    "                    n_correct += (train_predict == train_labels.to(device)).sum().item()                    \n",
    "        \n",
    "                    \n",
    "        acc = n_correct / (batch_size * len(train_loader))   \n",
    "        early_bird_ls.append(acc)\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        for test_batch, test_labels in test_loader: \n",
    "        \n",
    "            val_batch_count += 1\n",
    "            \n",
    "            if val_batch_count % 100 == 0 : \n",
    "                print(\"{}번째 Validation 배치가 돌고 있습니다.\".format(val_batch_count),end='\\r')\n",
    "            test_softmax = model(test_batch.to(device)).cpu()\n",
    "            \n",
    "            test_predict = test_softmax.argmax(dim=1)\n",
    "            val_n_correct += (test_predict == test_labels).sum().item()     \n",
    "            \n",
    "            loss = criterion(test_softmax, test_labels)\n",
    "                    \n",
    "        val_acc = val_n_correct / (len(test_loader) * batch_size)   \n",
    "        \n",
    "        query_idx_ls = np.random.choice(test_loader.dataset.X.shape[0], 1000, replace=False)\n",
    "        # random sampling\n",
    "        \n",
    "        train_loader = add_to_loader(train_loader,test_loader,query_idx_ls)\n",
    "        test_loader = pop_to_loader(test_loader,query_idx_ls)\n",
    "            \n",
    "        with open(\"english_random_sampling.txt\",'a') as f :\n",
    "            f.write('{}번째 iteration 의 Validation loss 는 {}입니다.\\n'.format(iteration,loss))\n",
    "            f.write('{}번째 iteration 의 Validation accuracy 는 {}입니다.\\n'.format(iteration,val_acc))\n",
    "            f.write(\"###################################################################\\n\")\n",
    "            \n",
    "#         if early_bird_ls[-1] < early_bird_ls[-2] : \n",
    "#             early_bird_count += 1\n",
    "            \n",
    "#             if early_bird_count > 5 : \n",
    "#                 break\n",
    "    return print(\"EARLY BIRD BREAK!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY BIRD BREAK!!치가 돌고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "train(model,train_iter,test_iter,epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
