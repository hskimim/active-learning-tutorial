{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from collections import defaultdict,Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([670000, 12, 25]), torch.Size([330000, 12, 25]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_idx_dict = load_obj('../data/yelp_word_to_idx_dict')\n",
    "idx_to_word_dict = {idx:word for (word,idx) in word_to_idx_dict.items()}\n",
    "\n",
    "train_X = torch.load('../data/yelp_train_X.pt')\n",
    "train_y = torch.load('../data/yelp_train_y.pt')\n",
    "test_X = torch.load('../data/yelp_test_X.pt')\n",
    "test_y = torch.load('../data/yelp_test_y.pt')\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1000, 12, 25]), torch.Size([90000, 12, 25]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = torch.cat((train_X[1000:],test_X)); test_y = torch.cat((train_y[1000:],test_y))\n",
    "train_X = train_X[:1000,:]; train_y = train_y[:1000]\n",
    "test_X = test_X[:90000]; test_y = test_y[:90000]\n",
    "\n",
    "train_X.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 100,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 10}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(train_X,train_y)\n",
    "train_iter = data.DataLoader(training_set, **params)\n",
    "\n",
    "testing_set = Dataset(test_X,test_y)\n",
    "test_iter = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained GloVe Embedding Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 단어의 길이 170007 개에서 GloVe 벡터로 초기화된 단어의 갯수는 83087 개입니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(170007, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract word embeddings from the Glove\n",
    "embeddings_index = dict()\n",
    "f = open('../data/glove.twitter.27B.200d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "matrix_len = len(word_to_idx_dict)\n",
    "weights_matrix = np.zeros((matrix_len, 200))\n",
    "words_found = 0\n",
    "\n",
    "for i, word in enumerate(word_to_idx_dict.keys()):\n",
    "    try: \n",
    "        weights_matrix[i] = embeddings_index[word]\n",
    "        words_found += 1\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=(200, ))\n",
    "        \n",
    "print(\"전체 단어의 길이 {} 개에서 GloVe 벡터로 초기화된 단어의 갯수는 {} 개입니다.\".format(len(word_to_idx_dict),words_found))\n",
    "\n",
    "weights_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordAttention(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,hidden_size) : \n",
    "        \n",
    "        super(WordAttention,self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Linear(hidden_size*2,hidden_size*2).to(device)\n",
    "        self.word_proj_params = nn.Parameter(torch.Tensor(hidden_size*2,1)).to(device)\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.word_proj_params)\n",
    "        \n",
    "    def forward(self,outputs) : \n",
    "        \n",
    "        outputs = outputs.permute(1,0,2) #[batch_size, sent_len, hidden_dim*2]\n",
    "\n",
    "        u = torch.tanh(self.linear(outputs)) #[batch_size, sent_len, hidden_dim*2]  \n",
    "        word_proj_params = self.word_proj_params.expand(self.batch_size,-1,-1) #[batch_size,hidden_dim*2,1]\n",
    "    \n",
    "        atten = torch.bmm(u,word_proj_params) #[batch_size,sent_len,1]\n",
    "        a = torch.softmax(atten,dim=1) #[batch_size,sent_len,1]\n",
    "        s = torch.sum(torch.mul(a,outputs),dim=1) #[batch_size,hidden_dim*2]\n",
    "        \n",
    "        return s,a\n",
    "\n",
    "def create_emb_layer(weights_matrix):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim,padding_idx = 1) # <pad>\n",
    "    emb_layer.weight = nn.Parameter(torch.tensor(weights_matrix,dtype=torch.float32))\n",
    "    \n",
    "    return emb_layer\n",
    "    \n",
    "class WordRNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len,weights_matrix) : \n",
    "        \n",
    "        super(WordRNN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.gru_hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.max_sent_len = max_sent_len\n",
    "#         self.embeddings = nn.Embedding(vocab_size,embed_size,padding_idx = 1).to(device)\n",
    "        self.embeddings = create_emb_layer(weights_matrix).to(device)\n",
    "        # GloVe 로 Initialize만 시키고, Training이 가능하게 해줍니다.\n",
    "        self.gru = nn.GRU(embed_size,hidden_size,num_layer,bidirectional=True).to(device)\n",
    "        \n",
    "        self.word_atten = WordAttention(batch_size,hidden_size).to(device)\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        for layer_p in self.gru._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    nn.init.xavier_normal_(self.gru.__getattr__(p),)\n",
    "\n",
    "    def forward(self,input_,hidden) : \n",
    "        \n",
    "        sent_vec_ls = []; word_attention_ls = []\n",
    "        \n",
    "        for i in range(self.max_sent_len) : \n",
    "            x = input_[:,i,:]  # x : [batch_size, T :(word length per sentence)]\n",
    "            embeds = self.embeddings(x).permute(1,0,2) # [T, batch_size, embed_dim] \n",
    "\n",
    "            outputs, hidden = self.gru(embeds,hidden)\n",
    "            \n",
    "            sent_vec,word_attention = self.word_atten(outputs)\n",
    "        \n",
    "            sent_vec_ls.append(sent_vec.unsqueeze(1))\n",
    "            word_attention_ls.append(word_attention.permute(0,2,1))\n",
    "        \n",
    "        sent_vec = torch.cat(sent_vec_ls,dim=1)\n",
    "        word_attention = torch.cat(word_attention_ls,dim=1)\n",
    "                \n",
    "        return sent_vec,word_attention,hidden\n",
    "    # [batch_size,sent_len,hidden_size]\n",
    "    # [batch_size,sent_len,word_len]\n",
    "    # [num_layer*bidirectional(2), batch_size, hidden_size]\n",
    "\n",
    "class SentAttention(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,hidden_size) : \n",
    "        \n",
    "        super(SentAttention,self).__init__() \n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Linear(hidden_size*2,hidden_size*2).to(device)\n",
    "        self.sent_proj_params = nn.Parameter(torch.Tensor(hidden_size*2,1)).to(device)\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        torch.nn.init.xavier_uniform_(self.linear.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.sent_proj_params)\n",
    "        \n",
    "    def forward(self,outputs) : \n",
    "        \n",
    "        outputs = outputs.permute(1,0,2) #[batch_size, doc_len, hidden_dim*2]\n",
    "        u = torch.tanh(self.linear(outputs)) #[batch_size, doc_len, hidden_dim*2]\n",
    "        sent_proj_params = self.sent_proj_params.expand(self.batch_size,-1,-1) #[batch_size,hidden_dim*2,1]\n",
    "        atten = torch.bmm(u,sent_proj_params) #[batch_size,doc_len,1]\n",
    "        a = torch.softmax(atten,dim=1) #[batch_size,doc_len,1]\n",
    "        v = torch.sum(a * outputs,dim=1) #[batch_size,hidden_dim*2]\n",
    "        return v,a\n",
    "\n",
    "class SentRNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer) : \n",
    "        \n",
    "        super(SentRNN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.gru_hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size*2,hidden_size,num_layer,bidirectional=True).to(device)\n",
    "        \n",
    "        self.sent_atten = SentAttention(batch_size,hidden_size)\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        for layer_p in self.gru._all_weights:\n",
    "            for p in layer_p:\n",
    "                if 'weight' in p:\n",
    "                    nn.init.xavier_normal_(self.gru.__getattr__(p),)\n",
    "\n",
    "    def forward(self,x,hidden) : \n",
    "        \n",
    "        x = x.permute(1,0,2) #x : [doc_len,batch_size, hidden*2]\n",
    "\n",
    "        outputs, hidden = self.gru(x,hidden)\n",
    "    \n",
    "        doc_vec,sent_attention = self.sent_atten(outputs)\n",
    "        \n",
    "        return doc_vec,sent_attention,hidden\n",
    "    \n",
    "    #[batch_size,hidden_dim*2]\n",
    "    #[batch_size,doc_len,1]\n",
    "    #[num_layer*2,batch_size,hidden_dim]\n",
    "\n",
    "class HAN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len,num_class,weights_matrix) : \n",
    "        \n",
    "        super(HAN,self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layer\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.num_class = num_class\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "        self.word_encoder =\\\n",
    "        WordRNN(batch_size,vocab_size,embed_size,hidden_size,num_layer,max_sent_len,weights_matrix).to(self.device)\n",
    "        \n",
    "        self.sent_encoder =\\\n",
    "        SentRNN(batch_size,vocab_size,embed_size,hidden_size,num_layer).to(self.device)\n",
    "        \n",
    "        self.proj_layer = nn.Linear(hidden_size*2,num_class).to(self.device)\n",
    "        self.initialize_weight()\n",
    "        \n",
    "    def initialize_weight(self) : \n",
    "        torch.nn.init.xavier_uniform_(self.proj_layer.weight)\n",
    "        \n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = \\\n",
    "        Variable(torch.zeros(self.num_layers*2, batch_size, self.hidden_size, device=self.device))\n",
    "            \n",
    "        return hidden\n",
    "    \n",
    "    def forward(self,input_) : \n",
    "        \n",
    "        (batch_size,sent_len,doc_len) = input_.size()\n",
    "        \n",
    "        word_encoder_hidden = self.init_hidden(batch_size)\n",
    "        sent_vec,word_attention,hidden = self.word_encoder(input_,word_encoder_hidden)\n",
    "        sent_vec = nn.LayerNorm(self.hidden_size*2).to(device)(sent_vec)\n",
    "        \n",
    "        sent_encoder_hidden = self.init_hidden(batch_size)\n",
    "        doc_vec,sent_attention,hidden = self.sent_encoder(sent_vec,sent_encoder_hidden)\n",
    "        doc_vec = nn.LayerNorm(self.hidden_size*2).to(device)(doc_vec)\n",
    "        \n",
    "        logit = self.proj_layer(doc_vec)\n",
    "        log_softmax = torch.log_softmax(logit,dim=1)\n",
    "        \n",
    "        return log_softmax, word_attention, sent_attention\n",
    "\n",
    "params = {'batch_size' : 100,\n",
    "'vocab_size' : len(word_to_idx_dict),\n",
    "'embed_size' : 200,\n",
    "'hidden_size' : 50,\n",
    "'num_layer' : 1,\n",
    "'max_sent_len' : 12,       \n",
    "'num_class' : 5,\n",
    "'weights_matrix' : weights_matrix,\n",
    "}\n",
    "\n",
    "model = HAN(**params).to(device)\n",
    "model.load_state_dict(torch.load('../data/yelp_HAN.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig and Testing with AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryStrategy() : \n",
    "        \n",
    "    def least_confidence(self,x,k=10,score=False) :\n",
    "\n",
    "        max_idx_ls = x.max(1)[0].cpu().data.numpy()\n",
    "        \n",
    "        if score : \n",
    "            return max_idx_ls\n",
    "        return list(np.argsort(1-max_idx_ls)[-k:])\n",
    "\n",
    "    def margin_sampling(self,x,k=10,score=False) : \n",
    "\n",
    "        sorted_x = torch.sort(x,dim=1)[0]\n",
    "\n",
    "        margin = sorted_x[:,-1] - sorted_x[:,-2]\n",
    "\n",
    "        if score : \n",
    "            return margin\n",
    "        \n",
    "        return list(np.argsort(margin.cpu().data.numpy())[-k:])\n",
    "\n",
    "    def entropy(self,x,k=10,score=False) : \n",
    "\n",
    "        entropy_ls = []\n",
    "\n",
    "        def calc_entropy(softmax) : \n",
    "\n",
    "            entropy = 0\n",
    "\n",
    "            for x in softmax : \n",
    "                entropy += -1 * np.exp(x.cpu().data.numpy()) * np.log2(np.exp(x.cpu().data.numpy()))\n",
    "                # exponential 을 취해준 이유는 모델의 반환값이 log softmax 이기 때문입니다.\n",
    "            return entropy\n",
    "\n",
    "        for i in x : \n",
    "            entropy_ls.append(calc_entropy(i))\n",
    "            \n",
    "        if score : \n",
    "            return entropy_ls\n",
    "        \n",
    "        return list(np.argsort(np.array(entropy_ls))[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_loader(train_loader,test_loader,idx_ls) : \n",
    "    \n",
    "    X = torch.cat((train_loader.dataset.X,test_loader.dataset.X[idx_ls]),dim=0)\n",
    "    y = torch.cat((train_loader.dataset.y,test_loader.dataset.y[idx_ls]),dim=0)\n",
    "    \n",
    "    params = {'batch_size': 100,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    training_set = Dataset(X,y)\n",
    "    train_iter = data.DataLoader(training_set, **params)\n",
    "    \n",
    "    return train_iter\n",
    "\n",
    "def pop_to_loader(loader,query_ls) : \n",
    "    \n",
    "    orgin_idx_ls = [i for i in range(loader.dataset.X.shape[0])]\n",
    "    \n",
    "    idx_ls = []\n",
    "    for val in orgin_idx_ls : \n",
    "        if val not in query_ls : \n",
    "            idx_ls.append(val)\n",
    "            \n",
    "    X = loader.dataset.X[idx_ls]\n",
    "    y = loader.dataset.y[idx_ls]\n",
    "    \n",
    "    params = {'batch_size': 100,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    # test_iter 는 shuffle 을 해서는 안됩니다. 그 이유는 validation datasets 에 대해서 softmax 값을 순차적으로 나열하고,\n",
    "    # 이에 따라, k-means 를 적용 및 informative instance를 쿼리하기 때문에, sequence 정보를 보존해야 합니다.\n",
    "    # 또한, 해당 데이터셋은 training 에는 적용되는 데이터가 아니기 때문에, shuffle 을 하지 않아도 무관합니다.\n",
    "    \n",
    "    testing_set = Dataset(X,y)\n",
    "    iter_ = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    return iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, lr = 0.01, batch_size = 100, epoch = 10) :\n",
    "    \n",
    "    query = QueryStrategy()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "    criterion_cpu = nn.NLLLoss()\n",
    "    \n",
    "    early_bird_ls = [0.0]\n",
    "    early_bird_count = 0\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    for _ in range(15) : \n",
    "        iteration += 1\n",
    "        val_batch_count = 0; batch_count = 0\n",
    "        n_correct = 0; val_n_correct = 0\n",
    "        score_ls = []\n",
    "        \n",
    "        train_length = train_loader.dataset.X.shape[0]\n",
    "        test_length = test_loader.dataset.X.shape[0]\n",
    "        with open(\"han_uncertainty_sampling.txt\",'a') as f : \n",
    "            f.write(\"training 데이터셋의 크기 : {} , validation 데이터셋의 크기 : {}\\n\".format(train_length,test_length))\n",
    "            f.write(\"###################################################################\\n\")\n",
    "        \n",
    "        if train_length > (train_length + test_length) * 0.6 : \n",
    "            with open(\"han_uncertainty_sampling.txt\",'a') as f : \n",
    "                f.write(\"training 데이터셋의 크기가 전체의 60%가 넘었기 때문에, iteration 을 종료합니다.\\n\")\n",
    "                \n",
    "            return\n",
    "        \n",
    "        model.train()        \n",
    "        \n",
    "        for time in range(epoch) : \n",
    "            for train_batch, train_labels in train_loader:  \n",
    "\n",
    "                batch_count += 1 \n",
    "                if batch_count % 100 == 0 and batch_count != 0: \n",
    "                    print(\"{}번째 Training 배치가 돌고 있습니다.\".format(batch_count),end='\\r')\n",
    "\n",
    "                train_softmax, _, _  = model(train_batch.to(device))\n",
    "                train_predict = train_softmax.argmax(dim=1)     \n",
    "\n",
    "                loss = criterion(train_softmax,train_labels.to(device))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if time == epoch - 1 : \n",
    "                    n_correct += (train_predict == train_labels.to(device)).sum().item()                    \n",
    "        \n",
    "                    \n",
    "        acc = n_correct / (batch_size * len(train_loader))   \n",
    "        early_bird_ls.append(acc)\n",
    "        with open(\"han_uncertainty_sampling.txt\",'a') as f : \n",
    "            f.write(\"{}번째 iteration 의 Training loss 는 {}입니다.\\n\".format(iteration,loss))\n",
    "            f.write(\"{}번째 iteration 의 Training accuracy 는 {}입니다.\\n\".format(iteration,acc))\n",
    "            f.write(\"###################################################################\\n\")\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        for test_batch, test_labels in test_loader: \n",
    "        \n",
    "            val_batch_count += 1\n",
    "            \n",
    "            if val_batch_count % 100 == 0 : \n",
    "                print(\"{}번째 Validation 배치가 돌고 있습니다.\".format(val_batch_count),end='\\r')\n",
    "            test_softmax, _, _  = model(test_batch.to(device))\n",
    "            \n",
    "            test_predict = test_softmax.argmax(dim=1)\n",
    "            val_n_correct += (test_predict.cpu() == test_labels).sum().item()     \n",
    "            \n",
    "            loss = criterion_cpu(test_softmax.cpu(), test_labels)\n",
    "            \n",
    "            test_score = list(query.margin_sampling(test_softmax.data,k=0,score=True).cpu().data.numpy())\n",
    "            score_ls += test_score\n",
    "            \n",
    "        val_acc = val_n_correct / (len(test_loader) * batch_size)\n",
    "        \n",
    "        query_idx_ls = np.argsort(score_ls)[-1000:]\n",
    "        val_acc = val_n_correct / (len(test_loader) * batch_size)   \n",
    "        train_loader = add_to_loader(train_loader,test_loader,query_idx_ls)\n",
    "        test_loader = pop_to_loader(test_loader,query_idx_ls)\n",
    "            \n",
    "        with open(\"han_uncertainty_sampling.txt\",'a') as f :\n",
    "            f.write('{}번째 iteration 의 Validation loss 는 {}입니다.\\n'.format(iteration,loss))\n",
    "            f.write('{}번째 iteration 의 Validation accuracy 는 {}입니다.\\n'.format(iteration,val_acc))\n",
    "            f.write(\"###################################################################\\n\")\n",
    "            \n",
    "        if early_bird_ls[-1] < early_bird_ls[-2] : \n",
    "            early_bird_count += 1\n",
    "            \n",
    "            if early_bird_count > 5 : \n",
    "                break\n",
    "    return print(\"EARLY BIRD BREAK!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY BIRD BREAK!!치가 돌고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "train(model,train_iter,test_iter,epoch=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine_3.6",
   "language": "python",
   "name": "engine_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
