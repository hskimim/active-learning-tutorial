{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "SEED = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx_dict = load_obj('word_to_idx_dict')\n",
    "word_to_freq_dict = load_obj('word_to_freq_dict')\n",
    "\n",
    "idx_to_word_dict = {idx:word for (word,idx) in word_to_idx_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = torch.load('train_X.pt')\n",
    "train_y = torch.load('train_y.pt')\n",
    "test_X = torch.load('test_X.pt')\n",
    "test_y = torch.load('test_y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        'Initialization'\n",
    "        self.y = y\n",
    "        self.X = X\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load data and get label\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        X = self.X[index]\n",
    "        y = self.y[index]\n",
    "\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 데이터셋의 67% 가 training 데이터셋, 33%가 validation 데이터셋이였습니다. 가정이 labeled data의 갯수가 적다는 가정이기 때문에, 이를 바꿔서 DataLoader 를 할당해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(train_X,train_y)\n",
    "test_iter = data.DataLoader(training_set, **params)\n",
    "\n",
    "testing_set = Dataset(test_X,test_y)\n",
    "train_iter = data.DataLoader(testing_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module) : \n",
    "    \n",
    "    def __init__(self,VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE ,KERNEL_SIZE, NUM_FILTER, N_CLASS ) : \n",
    "        super(CNN, self).__init__()\n",
    "        self.vocab_size = VOCAB_SIZE \n",
    "        self.embed_size = EMBED_SIZE \n",
    "        self.hid_size = HID_SIZE \n",
    "        self.dropout = DROPOUT \n",
    "        self.batch_size = BATCH_SIZE\n",
    "        if type(KERNEL_SIZE) !=list :\n",
    "            self.kernel_size = list(KERNEL_SIZE)\n",
    "        else : self.kernel_size = KERNEL_SIZE \n",
    "        self.num_filter = NUM_FILTER \n",
    "        self.num_class = N_CLASS \n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = self.vocab_size,\n",
    "            embedding_dim = self.embed_size,\n",
    "            padding_idx = 1) \n",
    "        \n",
    "        self.convs = nn.ModuleList([(nn.Conv2d(in_channels = 1,out_channels = self.num_filter,\\\n",
    "        kernel_size = (kernel,self.embed_size))) for kernel in self.kernel_size])\n",
    "        \n",
    "        self.fully_connect = nn.Sequential(\n",
    "        nn.Linear(self.num_filter * len(self.kernel_size),self.hid_size),nn.ReLU(),\n",
    "        nn.Dropout(self.dropout),nn.Linear(self.hid_size , self.num_class),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x) : \n",
    "        x = x.view(self.batch_size,-1) # [batch_size,max_length]\n",
    "        \n",
    "        embed = self.embedding(x) \n",
    "        embed = embed.unsqueeze(1)\n",
    "                \n",
    "        convolution = [conv(embed).squeeze(3) for conv in self.convs]\n",
    "        \n",
    "        pooled = [F.max_pool1d(conv,(conv.size(2))).squeeze(2) for conv in convolution]\n",
    "        \n",
    "        dropout = [F.dropout(pool,self.dropout) for pool in pooled]\n",
    "        \n",
    "        concatenate = torch.cat(dropout, dim = 1) \n",
    "        # [batch_size , num_filter * num_kernel]\n",
    "\n",
    "        logit = self.fully_connect(concatenate)\n",
    "        \n",
    "        return torch.softmax(logit,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word_to_idx_dict)\n",
    "EMBED_SIZE = 256\n",
    "HID_SIZE = 128\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "KERNEL_SIZE = [2,3,4,5]\n",
    "NUM_FILTER = 4\n",
    "N_CLASS = 5\n",
    "\n",
    "model = CNN(VOCAB_SIZE, EMBED_SIZE, HID_SIZE, DROPOUT, BATCH_SIZE, KERNEL_SIZE, NUM_FILTER, N_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig and Testing with AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QueryStrategy() : \n",
    "        \n",
    "    def least_confidence(self,x,k=10,score=False) :\n",
    "\n",
    "        max_idx_ls = x.max(1)[0].cpu().data.numpy()\n",
    "        \n",
    "        if score : \n",
    "            return max_idx_ls\n",
    "        return list(np.argsort(1-max_idx_ls)[-k:])\n",
    "\n",
    "    def margin_sampling(self,x,k=10,score=False) : \n",
    "\n",
    "        sorted_x = torch.sort(x,dim=1)[0]\n",
    "\n",
    "        margin = sorted_x[:,-1] - sorted_x[:,-2]\n",
    "\n",
    "        if score : \n",
    "            return margin\n",
    "        \n",
    "        return list(np.argsort(margin.cpu().data.numpy())[-k:])\n",
    "\n",
    "    def entropy(self,x,k=10,score=False) : \n",
    "\n",
    "        entropy_ls = []\n",
    "\n",
    "        def calc_entropy(softmax) : \n",
    "\n",
    "            entropy = 0\n",
    "\n",
    "            for x in softmax : \n",
    "                entropy += -1 * x.cpu().data.numpy() * np.log2(x.cpu().data.numpy())\n",
    "\n",
    "            return entropy\n",
    "\n",
    "        for i in x : \n",
    "            entropy_ls.append(calc_entropy(i))\n",
    "            \n",
    "        if score : \n",
    "            return entropy_ls\n",
    "        \n",
    "        return list(np.argsort(np.array(entropy_ls))[-k:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_loader(train_loader,test_loader,idx_ls) : \n",
    "    \n",
    "    X = torch.cat((train_loader.dataset.X,test_loader.dataset.X[idx_ls]),dim=0)\n",
    "    y = torch.cat((train_loader.dataset.y,test_loader.dataset.y[idx_ls]),dim=0)\n",
    "    \n",
    "    params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    # Generators\n",
    "    training_set = Dataset(X,y)\n",
    "    train_iter = data.DataLoader(training_set, **params)\n",
    "    \n",
    "    return train_iter\n",
    "\n",
    "def pop_to_loader(loader,query_ls) : \n",
    "    \n",
    "    orgin_idx_ls = np.array([i for i in range(loader.dataset.X.shape[0])])\n",
    "    not_int_ls = np.isin(orgin_idx_ls,query_ls,invert=True)\n",
    "    idx_ls = [i for i in not_int_ls if i]\n",
    "    \n",
    "    X = loader.dataset.X[idx_ls]\n",
    "    y = loader.dataset.y[idx_ls]\n",
    "    \n",
    "    params = {'batch_size': 50,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 10}\n",
    "\n",
    "    # Generators\n",
    "    testing_set = Dataset(X,y)\n",
    "    iter_ = data.DataLoader(testing_set, **params)\n",
    "    \n",
    "    return iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diverse Mini-Batch Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 알고리즘 정의\n",
    "- 전체 데이터 셋에 대한 softmax 값을 저장\n",
    "- pre-filter 데이터셋 확보 $\\beta k > k$\n",
    "- weighted-kmeans 를 계산 (sklearn Kmeans 사용)\n",
    "- centriod 와 각 instance 간의 distance 를 Euclidean distance를 통해 계산 및 sorting\n",
    "- 상위 $k$ 개의 인스턴스 index를 반환\n",
    "_________________________\n",
    "- $\\beta = 10$\n",
    "- $k = 100$\n",
    "- 이에 따라 $\\beta k$ 즉, 1000개를 pre-filter 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Diverse_AL() : \n",
    "    \n",
    "    def __init__(self,loader,score_ls,beta,k,rnd_state=123) : \n",
    "        \n",
    "        \"\"\"\n",
    "        loader : iterator for training or validation dataset \n",
    "        score_ls : container which save the uncertainty score such as margin sampling // dtype : list\n",
    "        beta : pre-filter hyperparameter\n",
    "        k : number of query instance        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.X_data = loader.dataset.X\n",
    "        self.batch = loader.dataset.X.view(self.X_data.shape[0],-1).cpu().data.numpy()        \n",
    "        self.score = torch.cat(score_ls).cpu().data.numpy()\n",
    "        self.k = k ; self.beta = beta; self.rnd_state = rnd_state\n",
    "        self.prefilter()\n",
    "        \n",
    "    def prefilter(self) : \n",
    "        \"\"\"\n",
    "        When run the kmeans algorithm, \n",
    "        if the number of distinct data is less than the number of clusters\n",
    "        add code that reduces the number of clusters by half \n",
    "        \"\"\"\n",
    "        self.prefiltered_batch = self.batch[np.argsort(self.score) < self.k * self.beta]\n",
    "        self.prefiltered_score = np.argpartition(self.score,-self.k * self.beta)[-self.k * self.beta:]\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"error\")\n",
    "            try : \n",
    "                kmeans = KMeans(n_clusters=self.k, random_state = self.rnd_state,n_jobs = 10)\\\n",
    "                    .fit(self.prefiltered_batch, sample_weight = self.prefiltered_score)\n",
    "            except : \n",
    "                kmeans = KMeans(n_clusters=self.k//2 , random_state = self.rnd_state,n_jobs = 10)\\\n",
    "                .fit(self.prefiltered_batch, sample_weight = self.prefiltered_score)\n",
    "            \n",
    "        self.clusters=kmeans.fit_predict(self.prefiltered_batch)\n",
    "        self.centroids = kmeans.cluster_centers_\n",
    "            \n",
    "    def return_instance(self) : \n",
    "\n",
    "        idx_ls = []; return_ls = []\n",
    "        np_array = self.prefiltered_batch.copy()\n",
    "\n",
    "        def _k_mean_distance(data, centroids, clusters):\n",
    "\n",
    "            '''\n",
    "            data : batch_data // dtype : numpy.array([])\n",
    "            centriod : kmeans.cluster_centers_ // [num_clusters,data_dimension]\n",
    "            clusters : label assigned to each batch // [batch_size,]\n",
    "            '''\n",
    "\n",
    "            label_ls = []; dist_ls = []\n",
    "\n",
    "            for idx in list(set(clusters)) :\n",
    "                label_ls.append(idx)\n",
    "                dist_ls.append(np.sqrt(((np_array[clusters==idx] - centroids[idx]) ** 2).sum(axis=1)))\n",
    "\n",
    "            return dict(zip(label_ls,dist_ls))\n",
    "\n",
    "        dist_dict = _k_mean_distance(np_array,self.centroids,self.clusters)\n",
    "\n",
    "        for key in list(dist_dict.keys()) : \n",
    "            idx_ls.append(np.argmin(dist_dict[key]))\n",
    "\n",
    "        for idx,key in zip(idx_ls,list(dist_dict.keys())) : \n",
    "\n",
    "            return_ls.append(np_array[self.clusters==key][idx])\n",
    "\n",
    "        return return_ls\n",
    "    \n",
    "    def query(self) : \n",
    "\n",
    "        query_idx_ls = []\n",
    "        query_ls = self.return_instance()\n",
    "\n",
    "        for idx1 in range(len(self.prefiltered_batch)) : \n",
    "            for idx2 in range(len(query_ls)) : \n",
    "                if all(self.prefiltered_batch[idx1] == query_ls[idx2]) : \n",
    "                    query_idx_ls.append(idx1)\n",
    "                \n",
    "        return query_idx_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, test_loader, lr = 0.01, batch_size = 50) :\n",
    "    \n",
    "    query = QueryStrategy()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    criterion_cpu = nn.CrossEntropyLoss()\n",
    "    \n",
    "    training_loss_ls = []; validation_loss_ls = []\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while True : \n",
    "        iteration += 1\n",
    "        batch_count = 0; val_batch_count = 0\n",
    "        score_ls = []\n",
    "        \n",
    "        train_length = train_loader.dataset.X.shape[0]\n",
    "        test_length = test_loader.dataset.X.shape[0]\n",
    "        print(\"training 데이터셋의 크기 : {} , validation 데이터셋의 크기 : {}\".format(train_length,test_length))\n",
    "        print(\"###################################################################\")\n",
    "\n",
    "        if train_length > 700000 : \n",
    "            print(\"training 데이터셋의 크기가 전체의 70%가 넘었기 때문에, iteration 을 종료합니다.\")\n",
    "            print(\"최종 Validation Loss : {}\".format(validation_loss_ls[-1])\n",
    "                 )\n",
    "        model.train()        \n",
    "        \n",
    "        for train_batch, train_labels in train_loader:  \n",
    "            \n",
    "            batch_count += 1 \n",
    "            if batch_count % 100 == 0 and batch_count != 0: \n",
    "                print(\"{}번째 Training 배치가 돌고 있습니다.\"\\\n",
    "                      .format(batch_count),end='\\r')\n",
    "                        \n",
    "            train_softmax = model(train_batch.to(device))\n",
    "            train_predict = train_softmax.argmax(dim=1)\n",
    "\n",
    "            loss = criterion(train_softmax,train_labels.to(device))\n",
    "            training_loss_ls.append(loss)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(\"{}번째 iteration 의 Training loss 는 {}입니다.\".format(iteration,loss))\n",
    "        print(\"###################################################################\")\n",
    "        \n",
    "        model.eval()\n",
    "\n",
    "        for test_batch, test_labels in test_loader: \n",
    "        \n",
    "            val_batch_count += 1\n",
    "            \n",
    "            if val_batch_count % 100 == 0 : \n",
    "                print(\"{}번째 Validation 배치가 돌고 있습니다.\".format(val_batch_count),end='\\r')\n",
    "            test_softmax = model(test_batch.to(device)).cpu()\n",
    "            \n",
    "            loss = criterion(test_softmax, test_labels)\n",
    "            validation_loss_ls.append(loss.item())\n",
    "            \n",
    "            test_score = query.margin_sampling(test_softmax.data,score=True)             \n",
    "            score_ls.append(test_score)\n",
    "        \n",
    "        mini_batch_al = Diverse_AL(test_loader,score_ls,10,100) # beta = 10, k = 10\n",
    "        query_idx_ls = mini_batch_al.query()\n",
    "        \n",
    "        train_loader = add_to_loader(train_loader,test_loader,query_idx_ls)\n",
    "        test_loader = pop_to_loader(test_loader,query_idx_ls)\n",
    "                \n",
    "        print('{}번째 interation 의 Validation loss 는 {}입니다.'.format(iteration,loss))\n",
    "        print(\"###################################################################\")\n",
    "    return training_loss_ls, validation_loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
